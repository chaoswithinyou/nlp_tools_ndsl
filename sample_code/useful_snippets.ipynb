{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMU6AISLgoY89ghXQbdUSr2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Vietnamese Word Segmentation (vncorenlp and UITws)"],"metadata":{"id":"YKtjpO1-gzA1"}},{"cell_type":"markdown","source":["vncorenlp"],"metadata":{"id":"S7QpP3tjhFQY"}},{"cell_type":"code","source":["!pip3 install py_vncorenlp"],"metadata":{"id":"Gv3LYh1XhKdi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import py_vncorenlp\n","import os\n","cwd = os.getcwd()\n","py_vncorenlp.download_model(save_dir=cwd)\n","rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir=cwd)"],"metadata":{"id":"e4QjgrKggyp3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for single text\n","segmented_text = rdrsegmenter.word_segment(text)"],"metadata":{"id":"R7A8NzhBhpNZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for list of sentences\n","segmented_texts = [rdrsegmenter.word_segment(comment)[0] for comment in dataset['train']['sentence']]"],"metadata":{"id":"twDN3L6DhMpW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["UITws"],"metadata":{"id":"8QmC92nchxjX"}},{"cell_type":"code","source":["!pip3 install --user gitdir\n","!/root/.local/bin/gitdir https://github.com/chaoswithinyou/nlp_tools_ndsl/tree/main/word_segmentation"],"metadata":{"id":"2Rx3T1BZhz6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from word_segmentation.UITws import word_segment"],"metadata":{"id":"wPsK9aUfh70s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for single text\n","word_segment(t, single_text=True)"],"metadata":{"id":"B2N-xYkNiBBt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for list of sentence\n","word_segment(t)"],"metadata":{"id":"iX5hon6yiBat"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Vietnamese Word2vec (PhoW2V)"],"metadata":{"id":"R1_KXf5kePMi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pte1zxu0dN26"},"outputs":[],"source":["# download w2v_vi_100dims\n","!gdown --id 1--bdQosASIlc3O4xN5IZ5vLfZ_yEdfBZ"]},{"cell_type":"code","source":["# download w2v_vi_300dims\n","!gdown --id 1rA7rGOEuhzfTzEiSiH-MiqWhQeR_ETTi"],"metadata":{"id":"5e7WHQghdXyL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PhoW2V_300dims = pickle.load(open(cwd+'/vietnamese_PhoW2V_300dims.pickle','rb'))"],"metadata":{"id":"4u5kAW0xddPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PhoW2V_100dims = pickle.load(open(cwd+'/vietnamese_PhoW2V_00dims.pickle','rb'))"],"metadata":{"id":"-IdsxW_qdmcL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#PhoW2V = [words, word2idx, vectors]\n","vietnamese_w2v = {w: PhoW2V[2][PhoW2V[1][w]] for w in PhoW2V[0]}"],"metadata":{"id":"kweuuE6Bdn7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def word2ids(sen_list, maxlen, vector_size):\n","    x = torch.zeros((len(sen_list),1,maxlen))\n","    for i in tqdm(range(len(sen_list))):\n","      doc = tokenize.word_tokenize(sen_list[i])\n","      fil_doc_w2index = []\n","      for word in doc:\n","        try:\n","          fil_doc_w2index.append(PhoW2V[1][word])\n","        except Exception:\n","          pass\n","      if len(fil_doc_w2index)<=maxlen:\n","        x[i] = torch.cat((torch.LongTensor(fil_doc_w2index),torch.zeros(maxlen-len(fil_doc_w2index)))).unsqueeze(0)\n","      else:\n","        x[i] = torch.LongTensor(fil_doc_w2index[-maxlen-1:-1]).unsqueeze(0)\n","    return x"],"metadata":{"id":"JPpZlSWYkTEf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = word2ids(sen_list = segmented_texts, maxlen = 50, vector_size = 100)"],"metadata":{"id":"-31byq9jl7Hl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#PhoBart"],"metadata":{"id":"XgV3IGliiXOZ"}},{"cell_type":"code","source":["!pip install transformers\n","from transformers import AutoModel, AutoTokenizer"],"metadata":{"id":"SVUzWlhbjVHW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word\")\n","bartpho_word = AutoModel.from_pretrained(\"vinai/bartpho-word\")"],"metadata":{"id":"bbXlWmH7iZsf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# segmentation_text are list of sentences(output of word segmentation), input_ids \n","input_ids = word_tokenizer(segmented_texts, return_tensors='pt', padding=True)['input_ids']"],"metadata":{"id":"YzNk7cSginAf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vector_size = 1024\n","bartpho_word = bartpho_word.to(device)\n","def forward(self, x):\n","    with torch.no_grad():\n","        x = bartpho_word(x).last_hidden_state.unsqueeze(1)"],"metadata":{"id":"9YTB8BEVi7FY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#List of sentences to 2d list of ids"],"metadata":{"id":"hRAbRWrjgUdl"}},{"cell_type":"code","source":["from tqdm import tqdm\n","from nltk import tokenize\n","\n","def word2ids(sen_list, word2idx, maxlen, vector_size):\n","    x = torch.zeros((len(sen_list),1,maxlen))\n","    for i in tqdm(range(len(sen_list))):\n","      doc = tokenize.word_tokenize(sen_list[i])\n","      fil_doc_w2index = []\n","      for word in doc:\n","        try:\n","          fil_doc_w2index.append(word2idx[word])\n","        except Exception:\n","          pass\n","      if len(fil_doc_w2index)<=maxlen:\n","        x[i] = torch.cat((torch.LongTensor(fil_doc_w2index),torch.zeros(maxlen-len(fil_doc_w2index)))).unsqueeze(0)\n","      else:\n","        x[i] = torch.LongTensor(fil_doc_w2index[-maxlen-1:-1]).unsqueeze(0)\n","    return x"],"metadata":{"id":"NIOWVCrrgTaC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#MyDataset class"],"metadata":{"id":"nfvBz8pUzo6M"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader"],"metadata":{"id":"Pw-M2jng0BWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","  def __init__(self, x, y):\n","    self.data = x\n","    self.labels = y\n","\n","  def __len__(self):\n","    return len(self.labels)\n","  \n","  def __getitem__(self, index):\n","    return self.data[index], self.labels[index]"],"metadata":{"id":"TVKxar5uzocz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = MyDataset(input_ids, train_dataset['sentiment'])\n","test_data = MyDataset(input_ids2, test_dataset['sentiment'])"],"metadata":{"id":"KpjpHjAc0K_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, num_workers=2)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True, num_workers=2)"],"metadata":{"id":"gvm2vjS00Kmk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Accuracy class and usage"],"metadata":{"id":"tnKOF1J60SJr"}},{"cell_type":"code","source":["class Accuracy:\n","    \"\"\"A class to keep track of the accuracy while training\"\"\"\n","    def __init__(self):\n","        self.correct = 0\n","        self.total = 0\n","        \n","    def reset(self):\n","        \"\"\"Resets the internal state\"\"\"\n","        self.correct = 0\n","        self.total = 0\n","        \n","    def update(self, outputs, labels):\n","        \"\"\"\n","        Updates the internal state to later compute the overall accuracy\n","        \n","        output: the output of the network for a batch\n","        labels: the target labels\n","        \"\"\"\n","        _, predicted = torch.max(outputs.data, 1) # predicted now contains the predicted class index/label\n","        \n","        self.total += labels.size(0)\n","        self.correct += (predicted == labels).sum().item() # .item() gets the number, not the tensor\n","        #self.correct += ((outputs.data > 0.5) == labels).sum().item()\n","\n","    def compute(self):\n","        return self.correct/self.total"],"metadata":{"id":"KFVJZUbQ0cSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_accu():\n","    net.eval()\n","    test_accuracy = Accuracy()\n","    test_accuracy.reset()        \n","    with torch.no_grad():\n","        for test_data in test_loader:\n","            # get the data points\n","            test_inputs, test_labels = test_data\n","            test_inputs, test_labels = test_inputs.to(device).long(), test_labels.to(device)\n","            # forward the data through the network\n","            test_outputs = net(test_inputs)\n","            \n","            test_accuracy.update(test_outputs, test_labels)\n","            \n","    print(\"\\nTesting Accuracy: {:.2f}%\".format(100 * test_accuracy.compute()))"],"metadata":{"id":"EzLPHA150foR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model ini and training"],"metadata":{"id":"DAz-SjyH0hM_"}},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","bartpho_word = bartpho_word.to(device)\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels = 1,\n","                                out_channels = 100,\n","                                kernel_size = (2,vector_size))\n","        self.conv2 = nn.Conv2d(in_channels = 1,\n","                                out_channels = 100,\n","                                kernel_size = (3,vector_size))\n","        self.conv3 = nn.Conv2d(in_channels = 1,\n","                                out_channels = 100,\n","                                kernel_size = (4,vector_size))\n","        self.conv4 = nn.Conv2d(in_channels = 1,\n","                                out_channels = 100,\n","                                kernel_size = (5,vector_size))\n","        self.fc1 = nn.Linear(400, 3)\n","        self.dropout = nn.Dropout(0.30)\n","        self.dropout1 = nn.Dropout(0.30)\n","        self.dropout2 = nn.Dropout(0.30)\n","        self.dropout3 = nn.Dropout(0.30)\n","        self.dropout4 = nn.Dropout(0.30)\n","        self.dropout5 = nn.Dropout(0.30)\n","    def forward(self, x):\n","        with torch.no_grad():\n","            x = bartpho_word(x).last_hidden_state.unsqueeze(1)\n","        x = self.dropout(x)\n","        x1 = F.relu(self.conv1(x))\n","        x1 = self.dropout1(F.max_pool2d(x1, (x1.size()[-2],1)))\n","        x2 = F.relu(self.conv2(x))\n","        x2 = self.dropout2(F.max_pool2d(x2, (x2.size()[-2],1)))\n","        x3 = F.relu(self.conv3(x))\n","        x3 = self.dropout3(F.max_pool2d(x3, (x3.size()[-2],1)))\n","        x4 = F.relu(self.conv4(x))\n","        x4 = self.dropout4(F.max_pool2d(x4, (x4.size()[-2],1)))\n","        x = torch.flatten(torch.cat((x1,x2,x3,x4),-3),start_dim=-3)\n","        x = self.fc1(self.dropout5(x))\n","        return x\n","\n","net = Net()\n","net = net.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","#optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.5)"],"metadata":{"id":"ldY5wFho0g2R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","class LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(LSTM, self).__init__()\n","        self.num_layers =  num_layers\n","        self.hidden_size = hidden_size\n","\n","        #self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n","        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","\n","        # x -> (batch_size, sequence_length, input_size) because batch_size = true\n","        self.fc1 = nn.Linear(hidden_size, 32)\n","        self.fc2 = nn.Linear(32, num_classes)\n","        #self.fc = nn.Linear(hidden_size, num_classes)\n","        self.dropout1 = nn.Dropout(0.30)\n","        self.dropout2 = nn.Dropout(0.30)\n","\n","    def forward(self, x):\n","        # initial hidden state size is always (num_layer, batch_size, hidden_size)\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        #out, _ = self.rnn(x, h0)\n","        #out, _ = self.gru(x, h0)\n","        out, _ = self.lstm(x, (h0,c0))\n","        \n","        # out -> (batch_size, sequence_length, hidden_size) because batch_size = true\n","        out = out[:, -1, :] # only the last time step\n","\n","        out = F.relu(self.fc1(self.dropout1(out)))\n","        out = self.fc2(self.dropout2(out))\n","        #out = self.fc(self.dropout1(out))\n","        return out\n","\n","net = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.5)"],"metadata":{"id":"2B1SDbdh1G5O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","class CNNLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(CNNLSTM, self).__init__()\n","\n","        self.weights = ResNet50_Weights.DEFAULT\n","        self.preprocess = self.weights.transforms()\n","        self.pretrained_cnn = resnet50(weights=self.weights)\n","\n","        #self.resnet.fc = nn.Sequential(nn.Linear(self.resnet.fc.in_features, 300))\n","        self.pretrained_cnn.fc = nn.Linear(self.pretrained_cnn.fc.in_features, input_size)\n","\n","        self.num_layers =  num_layers\n","        self.hidden_size = hidden_size\n","\n","\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","\n","        # x -> (batch_size, sequence_length, input_size) because batch_size = true\n","        self.fc1 = nn.Linear(hidden_size, 32)\n","        self.fc2 = nn.Linear(32, num_classes)\n","        self.dropout = nn.Dropout(0.40)\n","\n","    def forward(self, x):\n","        hidden = None\n","        for i in range(x.size(1)):\n","            with torch.no_grad():\n","                out = self.pretrained_cnn(self.preprocess(x[:,i,:,:,:]))\n","            out, hidden = self.lstm(out.unsqueeze(1), hidden)\n","        \n","        # out -> (batch_size, sequence_length, hidden_size) because batch_size = true\n","        out = out[:, -1, :] # only the last time step\n","\n","        out = F.relu(self.fc1(out))\n","        out = self.fc2(self.dropout(out))\n","\n","        return out\n","\n","net = CNNLSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.5)"],"metadata":{"id":"60ALb85T1CxM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#net = net.float()\n","net.train()\n","live_accuracy = Accuracy()\n","\n","for epoch in range(100):  # loop over the dataset multiple times\n","    print(\"\\nStarting epoch {}\".format(epoch+1))\n","\n","    live_accuracy.reset()\n","    total = 0\n","    running_loss = 0.0\n","\n","    # to make a beautiful progress bar\n","    loader = tqdm(enumerate(train_loader), total=len(train_loader))\n","    for i, data in loader:\n","        # get the data points\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device).long(), labels.to(device)\n","        # zero the parameter gradients (else, they are accumulated)\n","        optimizer.zero_grad()\n","\n","        # forward the data through the network\n","        outputs = net(inputs)\n","        # calculate the loss given the output of the network and the target labels\n","        loss = criterion(outputs, labels)\n","        # calculate the gradients of the network w.r.t. its parameters\n","        loss.backward()\n","        # Let the optimiser take an optimization step using the calculated gradients\n","        optimizer.step()\n","        \n","        running_loss += loss\n","        total += outputs.size(0)\n","\n","        live_accuracy.update(outputs, labels)\n","        loader.set_description(\"loss: {:.5f}|acc: {:.2f}%\".format(running_loss/total,100 * live_accuracy.compute()))\n","    test_accu()\n","    net.train()\n","\n","print('Finished Training')"],"metadata":{"id":"zc-8G3h80oR8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Visualization"],"metadata":{"id":"aVqU0ksz0tIT"}},{"cell_type":"code","source":["# in test mode, we should set the 'learning_phase' flag to 0 (we don't want to use dropout)\n","get_doc_embedding = copy.deepcopy(net)\n","get_doc_embedding.fc1 = torch.nn.Identity()\n","get_doc_embedding.eval()\n","\n","num_sen = 1000\n","print('plotting embeddings of first',num_sen,'documents')\n","\n","doc_emb = get_doc_embedding(test_data.data[:num_sen].long()).detach().numpy()\n","\n","my_pca = PCA(n_components=10)\n","my_tsne = TSNE(n_components=2,perplexity=30)\n","doc_emb_pca = my_pca.fit_transform(doc_emb)\n","doc_emb_tsne = my_tsne.fit_transform(doc_emb_pca) \n","#doc_emb_tsne = my_tsne.fit_transform(doc_emb)\n","\n","labels_plt = test_data.labels[:num_sen]\n","my_colors = ['red','yellow','green']\n","\n","fig, ax = plt.subplots()\n","\n","for label in list(set(labels_plt)):\n","    idxs = [idx for idx,elt in enumerate(labels_plt) if elt==label]\n","    ax.scatter(doc_emb_tsne[idxs,0], \n","               doc_emb_tsne[idxs,1], \n","               c = my_colors[label],\n","               label=str(label),\n","               alpha=0.7,\n","               s=10)\n","\n","ax.legend(scatterpoints=1)\n","fig.suptitle('t-SNE visualization of CNN-based doc embeddings \\n (first 1000 docs from test set)',fontsize=10)\n","fig.set_size_inches(6,4)\n","#fig.savefig(path_to_plot + 'doc_embeddings_init.pdf',bbox_inches='tight')\n","fig.show()"],"metadata":{"id":"jJO96w5A0vD9"},"execution_count":null,"outputs":[]}]}