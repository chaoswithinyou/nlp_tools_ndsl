{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32312,"status":"ok","timestamp":1663347741898,"user":{"displayName":"Lam","userId":"17841603162440265686"},"user_tz":-420},"id":"dL72O0FtqJcD","outputId":"5a140ffe-a8e6-46b4-de40-411f398f8ed0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.7/dist-packages (0.5.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp) (4.64.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp) (1.21.6)\n","Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["!pip install pytorch-nlp\n","from torchnlp.datasets import imdb_dataset\n","from tqdm import tqdm\n","import re\n","from nltk import tokenize\n","import nltk\n","import pickle\n","from google.colab import drive\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import random\n","import numpy as np\n","################################################################################\n","drive.mount('/content/drive', force_remount=True)\n","nltk.download('punkt')\n","train = imdb_dataset(train=True)\n","test = imdb_dataset(test=True)\n","################################################################################\n","def tokenizing(data, mode=0):\n","    if mode==1:\n","        text = ' '.join([s[\"text\"] for s in data])\n","    else:\n","        text = data\n","    text = text.replace('<br /><br />',' ')\n","    text = text.replace('\\x85','')\n","    text = text.lower()\n","    tokenized_sentence = re.compile('[.!?()]').split(text)\n","    tokenized_corpus = [None]*len(tokenized_sentence)\n","    if mode==1:\n","        for i in tqdm(range(len(tokenized_sentence))):\n","            tokenized_corpus[i] = tokenize.word_tokenize(tokenized_sentence[i])\n","    else:\n","        for i in range(len(tokenized_sentence)):\n","            tokenized_corpus[i] = tokenize.word_tokenize(tokenized_sentence[i])\n","    return tokenized_corpus\n","################################################################################\n","class MyDataset(Dataset):\n","  def __init__(self, x, y, z):\n","    self.data = x\n","    self.labels = y\n","    self.lengths = z\n","\n","  def __len__(self):\n","    return len(self.labels)\n","  \n","  def __getitem__(self, index):\n","    return self.data[index], self.labels[index], self.lengths[index]\n","################################################################################\n","def custom_collate_fn(data):\n","    \"\"\"\n","       data: is a list of tuples with (example, label, length)\n","             where 'example' is a tensor of arbitrary shape\n","             and label/length are scalars\n","    \"\"\"\n","    _, labels, lengths = zip(*data)\n","    max_len = max(lengths)\n","    fix_size = data[0][0].size(1)\n","    features = torch.zeros((len(data), 1, fix_size, max_len))\n","    labels = torch.tensor(labels)\n","    lengths = torch.tensor(lengths)\n","\n","    for i in range(len(data)):\n","      k, j = data[i][0][0].size(0), data[i][0][0].size(1)\n","      features[i][0] = torch.cat((data[i][0][0], torch.zeros((k, max_len - j))),1)\n","\n","    return features.float(), labels.long(), lengths.long()\n","################################################################################\n","class Accuracy:\n","    \"\"\"A class to keep track of the accuracy while training\"\"\"\n","    def __init__(self):\n","        self.correct = 0\n","        self.total = 0\n","        \n","    def reset(self):\n","        \"\"\"Resets the internal state\"\"\"\n","        self.correct = 0\n","        self.total = 0\n","        \n","    def update(self, output, labels):\n","        \"\"\"\n","        Updates the internal state to later compute the overall accuracy\n","        \n","        output: the output of the network for a batch\n","        labels: the target labels\n","        \"\"\"\n","        # _, predicted = torch.max(output.data, 1) # predicted now contains the predicted class index/label\n","        \n","        self.total += labels.size(0)\n","        # self.correct += (predicted == labels).sum().item() # .item() gets the number, not the tensor\n","        self.correct += ((outputs.data > 0.5) == labels).sum().item()\n","\n","    def compute(self):\n","        return self.correct/self.total\n","################################################################################\n","def vec(word):\n","  try:\n","    a = model.wv[word]\n","  except:\n","    a = np.zeros(300)\n","  return a"]},{"cell_type":"code","source":["model = pickle.load(open('/content/drive/MyDrive/NLP/w2v_model_gensim.p','rb'))"],"metadata":{"id":"qbiwDcCwOUNV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = pickle.load(open('/content/drive/MyDrive/NLP/glove-wiki-gigaword-300.p','rb'))"],"metadata":{"id":"qnAMsaoePzWU","executionInfo":{"status":"ok","timestamp":1663347746322,"user_tz":-420,"elapsed":4427,"user":{"displayName":"Lam","userId":"17841603162440265686"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gYzOZkDAlceA","outputId":"b8b03f25-e20e-4c83-8561-1c8772a3e588"},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/25000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:100: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"," 63%|██████▎   | 15636/25000 [00:57<00:41, 227.17it/s]"]}],"source":["random.shuffle(train)\n","vector_size = 300\n","x = []\n","y = []\n","z = []\n","for a in tqdm(train):\n","  doc = tokenize.word_tokenize(a['text'])\n","  x.append(torch.reshape(torch.tensor(np.array([vec(word) for word in doc])), (1,vector_size, len(doc))))\n","  y.append([int(a['sentiment']=='pos')])\n","  z.append(len(doc))\n","train_dataset = MyDataset(x,y,z)\n","\n","random.shuffle(test)\n","x = []\n","y = []\n","z = []\n","for a in tqdm(test[0:1000]):\n","  doc = tokenize.word_tokenize(a['text'])\n","  x.append(torch.reshape(torch.tensor(np.array([vec(word) for word in doc])), (1,vector_size, len(doc))))\n","  y.append([int(a['sentiment']=='pos')])\n","  z.append(len(doc))\n","val_dataset = MyDataset(x,y,z)"]},{"cell_type":"code","source":["random.shuffle(train)\n","vector_size = 300\n","x = []\n","y = []\n","z = []\n","for a in tqdm(train):\n","  doc = tokenize.word_tokenize(a['text'])\n","  x.append(torch.reshape(torch.tensor(np.array([vec(word) for word in doc])), (1,vector_size, len(doc))))\n","  y.append([int(a['sentiment']=='pos')])\n","  z.append(len(doc))\n","train_dataset = MyDataset(x,y,z)\n","\n","random.shuffle(test)\n","x = []\n","y = []\n","z = []\n","for a in tqdm(test[0:1000]):\n","  doc = tokenize.word_tokenize(a['text'])\n","  x.append(torch.reshape(torch.tensor(np.array([vec(word) for word in doc])), (1,vector_size, len(doc))))\n","  y.append([int(a['sentiment']=='pos')])\n","  z.append(len(doc))\n","val_dataset = MyDataset(x,y,z)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xk105sbsM1AU","outputId":"e8f91e9f-560d-43df-81cf-277c723158bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/25000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:100: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"," 65%|██████▍   | 16242/25000 [00:53<00:26, 327.10it/s]"]}]},{"cell_type":"code","source":["a =  torch.zeros(2,2,2)\n","a[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMOwYMD5P5r_","executionInfo":{"status":"ok","timestamp":1663347866228,"user_tz":-420,"elapsed":8,"user":{"displayName":"Lam","userId":"17841603162440265686"}},"outputId":"644101c7-2c53-492d-e049-70aa0f731f36"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0.],\n","        [0., 0.]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(train_dataset, collate_fn=custom_collate_fn, batch_size=50, shuffle=True, num_workers=2)\n","val_loader = torch.utils.data.DataLoader(val_dataset, collate_fn=custom_collate_fn, batch_size=50, shuffle=True, num_workers=2)"],"metadata":{"id":"deChIDGQQreH","executionInfo":{"status":"ok","timestamp":1663345866231,"user_tz":-420,"elapsed":17,"user":{"displayName":"Lam","userId":"17841603162440265686"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":508,"status":"ok","timestamp":1663345975441,"user":{"displayName":"Lam","userId":"17841603162440265686"},"user_tz":-420},"id":"R2XXwFPmm095"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels = 1,\n","                                out_channels = 200,\n","                                kernel_size = (vector_size,2))\n","        self.conv2 = nn.Conv2d(in_channels = 1,\n","                                out_channels = 200,\n","                                kernel_size = (vector_size,3))\n","        self.conv3 = nn.Conv2d(in_channels = 1,\n","                                out_channels = 200,\n","                                kernel_size = (vector_size,4))\n","        self.conv4 = nn.Conv2d(in_channels = 1,\n","                                out_channels = 200,\n","                                kernel_size = (vector_size,5))\n","        self.fc1 = nn.Linear(800, 1)\n","        self.dropout = nn.Dropout(0.50)\n","    def forward(self, x):\n","        x1 = F.relu(self.conv1(x))\n","        x1 = F.max_pool2d(x1, (1,x1.size()[-1]))\n","        x2 = F.relu(self.conv2(x))\n","        x2 = F.max_pool2d(x2, (1,x2.size()[-1]))\n","        x3 = F.relu(self.conv3(x))\n","        x3 = F.max_pool2d(x3, (1,x3.size()[-1]))\n","        x4 = F.relu(self.conv4(x))\n","        x4 = F.max_pool2d(x4, (1,x4.size()[-1]))\n","        x = torch.flatten(torch.cat((x1,x2,x3,x4),1),start_dim=1)\n","        x = self.fc1(self.dropout(x))\n","        return x\n","\n","net = Net()\n","net = net.to(device)\n","criterion = nn.BCEWithLogitsLoss()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4u4HH-yEm3d2","outputId":"f6caa9b3-d681-42da-dc96-6cc19c0423d4","executionInfo":{"status":"error","timestamp":1663346197626,"user_tz":-420,"elapsed":218865,"user":{"displayName":"Lam","userId":"17841603162440265686"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 1\n"]},{"output_type":"stream","name":"stderr","text":["loss: 0.01425|acc: 49.42%: 100%|██████████| 100/100 [00:14<00:00,  6.96it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 2\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.01248|acc: 59.82%: 100%|██████████| 100/100 [00:15<00:00,  6.65it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 3\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.01050|acc: 71.34%: 100%|██████████| 100/100 [00:16<00:00,  6.09it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 4\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.00870|acc: 79.10%: 100%|██████████| 100/100 [00:15<00:00,  6.57it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 5\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.00719|acc: 84.66%: 100%|██████████| 100/100 [00:14<00:00,  6.86it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 6\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.00606|acc: 86.96%: 100%|██████████| 100/100 [00:14<00:00,  6.84it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 7\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.00508|acc: 89.16%: 100%|██████████| 100/100 [00:14<00:00,  6.94it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 8\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.00445|acc: 90.90%: 100%|██████████| 100/100 [00:14<00:00,  6.90it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 9\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.00392|acc: 91.84%: 100%|██████████| 100/100 [00:15<00:00,  6.66it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 10\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.00372|acc: 92.16%: 100%|██████████| 100/100 [00:14<00:00,  6.77it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 11\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.00347|acc: 92.92%: 100%|██████████| 100/100 [00:14<00:00,  6.95it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 12\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.00360|acc: 92.24%: 100%|██████████| 100/100 [00:14<00:00,  6.81it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 13\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.00309|acc: 93.60%: 100%|██████████| 100/100 [00:15<00:00,  6.44it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 14\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.00321|acc: 93.48%: 100%|██████████| 100/100 [00:14<00:00,  6.84it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 15\n"]},{"output_type":"stream","name":"stderr","text":["\n","loss: 0.00294|acc: 94.70%:  43%|████▎     | 43/100 [00:07<00:10,  5.48it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-36689ade1101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss: {:.5f}|acc: {:.2f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-b8c5e0ea409c>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, output, labels)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# self.correct += (predicted == labels).sum().item() # .item() gets the number, not the tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.5)\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","net = net.float()\n","net.train()\n","accuracy = Accuracy()\n","\n","for epoch in range(100):  # loop over the dataset multiple times\n","    print(\"\\nStarting epoch {}\".format(epoch+1))\n","\n","    accuracy.reset()\n","    total = 0\n","    running_loss = 0.0\n","\n","    # to make a beautiful progress bar\n","    loader = tqdm(enumerate(train_loader), total=len(train_loader))\n","    for i, data in loader:\n","        # get the data points\n","        inputs, labels, lengths = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        # zero the parameter gradients (else, they are accumulated)\n","        optimizer.zero_grad()\n","\n","        # forward the data through the network\n","        outputs = net(inputs.float())\n","        # calculate the loss given the output of the network and the target labels\n","        loss = criterion(outputs, labels.float())\n","        # calculate the gradients of the network w.r.t. its parameters\n","        loss.backward()\n","        # Let the optimiser take an optimization step using the calculated gradients\n","        optimizer.step()\n","        \n","        running_loss += loss\n","        total += outputs.size(0)\n","\n","        accuracy.update(outputs, labels)\n","        loader.set_description(\"loss: {:.5f}|acc: {:.2f}%\".format(running_loss/total,100 * accuracy.compute()))\n","\n","print('Finished Training')"]},{"cell_type":"code","source":["pickle.dump(net,open('/content/drive/MyDrive/NLP/imdb_sentiment.p','wb'))"],"metadata":{"id":"BkpPeqMfi_2l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = pickle.load(open('/content/drive/MyDrive/NLP/imdb_sentiment.p','rb'))"],"metadata":{"id":"QTKCFS0EjLmA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"id":"KLuV4Acmm6QU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663346262822,"user_tz":-420,"elapsed":14124,"user":{"displayName":"Lam","userId":"17841603162440265686"}},"outputId":"5f3cb3a0-9403-4b9e-b473-a43a4100c323"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:13<00:00,  7.34it/s]"]},{"output_type":"stream","name":"stdout","text":["Training Accuracy: 100.00%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["net.eval()\n","accuracy = Accuracy()\n","accuracy.reset()\n","# Gradients are calculated on the forward pass for every iteration.\n","# As we do not need gradients now, we can disable the calculation.\n","with torch.no_grad():\n","    for data in tqdm(train_loader):\n","        # get the data points\n","        inputs, labels, lengths = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        # forward the data through the network\n","        outputs = net(inputs.float())\n","        \n","        accuracy.update(outputs, labels)\n","\n","print(\"Training Accuracy: {:.2f}%\".format(100 * accuracy.compute()))"]},{"cell_type":"code","source":["net.eval()\n","accuracy = Accuracy()\n","accuracy.reset()        \n","with torch.no_grad():\n","    for data in tqdm(val_loader):\n","        # get the data points\n","        inputs, labels, lengths = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        # forward the data through the network\n","        outputs = net(inputs.float())\n","        \n","        accuracy.update(outputs, labels)\n","        \n","print(\"\\nTesting Accuracy: {:.2f}%\".format(100 * accuracy.compute()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fTX3Ue_2QU1t","executionInfo":{"status":"ok","timestamp":1663346239272,"user_tz":-420,"elapsed":2782,"user":{"displayName":"Lam","userId":"17841603162440265686"}},"outputId":"5fa3447c-40b4-499f-e4f6-fb1b4f6ed1f6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 20/20 [00:02<00:00,  8.05it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Testing Accuracy: 53.00%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1yYE4Wigl28XOxQhO-WOVW6-q5BzOMnl8","timestamp":1663185324787},{"file_id":"1mQAurto3EVGoG1RP2XU7I624AXaacgmx","timestamp":1660668415917},{"file_id":"1D1w4S3z06bVH05AqJwBa9aFf5CzcDiK4","timestamp":1660548913907}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}