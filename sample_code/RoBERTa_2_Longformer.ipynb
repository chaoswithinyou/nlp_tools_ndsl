{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4261,"status":"ok","timestamp":1666106935991,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"},"user_tz":-420},"id":"9nyCeH10tQ3i","outputId":"fcc51123-807d-4b77-fa29-a343f8f4cd1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Name: transformers\n","Version: 4.23.1\n","Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n","Home-page: https://github.com/huggingface/transformers\n","Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n","Author-email: transformers@huggingface.co\n","License: Apache\n","Location: /usr/local/lib/python3.7/dist-packages\n","Requires: importlib-metadata, numpy, tokenizers, filelock, huggingface-hub, tqdm, requests, pyyaml, packaging, regex\n","Required-by: \n"]}],"source":["!pip show transformers"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11631,"status":"ok","timestamp":1666183756819,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"},"user_tz":-420},"id":"SdMTCyK6xWHz","outputId":"615108c2-6f63-4a43-b7a5-923967a493cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.3 MB 37.9 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.6 MB 77.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 163 kB 92.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"]}],"source":["!pip install -U transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22524,"status":"ok","timestamp":1666183784784,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"},"user_tz":-420},"id":"5np9v8c1y4FV","outputId":"c60078c8-832a-44bd-d296-bd3fe2a53b12"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSmF_lpEvOnR"},"outputs":[],"source":["#!pip install transformers==3.0.2"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"P7ApP_6EvprQ","executionInfo":{"status":"ok","timestamp":1666183790525,"user_tz":-420,"elapsed":5759,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"}}},"outputs":[],"source":["import logging\n","import os\n","import math\n","import copy\n","import torch\n","from dataclasses import dataclass, field\n","from transformers import RobertaForMaskedLM, RobertaTokenizerFast, TextDataset, DataCollatorForLanguageModeling, Trainer\n","from transformers import TrainingArguments, HfArgumentParser\n","from transformers.models.longformer.modeling_longformer import LongformerSelfAttention\n","from transformers import AutoTokenizer\n","\n","logging.getLogger().setLevel(logging.INFO)\n","logger = logging.getLogger(__name__)\n","#logging.basicConfig(level=logging.INFO)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162,"referenced_widgets":["a303469db2d64d16a61a6ffadf6dca4f","ffc9e3081f8e465d9593d5bc4155f9b9","e0d10dcfb5ba4415badf770df6a2a83d","296cf74359964a5da8539222b555f2fa","6fb3b281bb3f4245af22d4ff9444db91","c69c36cc1e4d42ee9b969477a351394e","1efea6b592bc4146aab2251d36af5f7f","f196c9c905394386a4541ac5fd826c8f","3e06499e927a4cdaba6ed7957ff9e402","85f6601a3772461895722c0d58a8bbe9","f06c1f6b5f5d49b5832e950deaa23203","768f249be3cc4a48bfa0004f7129764b","55637dcaa88f439788d3b69919d4a2e9","fbcefcf8d7dc4aa88e0129b14dfad2bd","1e0c6d23539847a1977f222ca785144e","081e8cc2d0d3407587e24d29b6b31fd4","d198507b83c24dbe874f3c1b5898d314","3ddf5f077895482a8b0f613e159c8b7d","457a4618851546c4a0a015284e08e880","4bf25c6e3fd34c0d909ca65e7d79eb90","a66640340c20455aad7f2122a75d0e67","f4da6207d39a4f5ba8534b0f1c7d04c3","b28f96014ef14719a75a363825e2a563","75096a73ba4c48e49c5cfb73a57ff613","491f722ec598490bbe5af9f0d45699d4","73e11c0b37f04caf80007c8f4d50cdde","6b172bf44f7e422a9fc634aa5144db58","55327c2fe9bb4b5c88a394ec317ec9a4","39719bd81ec74d8088343df0f9f47023","916b3ba5b3564025b8700de93d7afc66","01c62d0f559f4b3b860a5754027513a6","5ffbc99d12ef4985b31fafb25154b28f","9208c5eb32bb45d488d3661a70ee0b95","bc0f939e158e4526a4d7b28f6e3e4e16","fd016e04ee18492fbb303fb4d71bc8dd","1bb365e6840c4dc88bca7de3bc8fe93b","3d98f4e3067c4ab6b5fb647b508a107b","edee4039146e44e881ea36c8d0938c5a","faeeb42120314eb99a1dea8daf338257","c34f79e2617045e6bf94b47df031f3b1","2416d1a36eb54b74836a69a15ccf220d","18ba6524105f43f9955563e2fd04afd8","599b7897741c4791a7316d83d81b0fba","7e9f6134a1bd468d8477194d32074906"]},"executionInfo":{"elapsed":23008,"status":"ok","timestamp":1666183813525,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"},"user_tz":-420},"id":"j3kVvsbvxSxT","outputId":"baeebb23-f906-4030-999f-afe62601d8c3"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/557 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a303469db2d64d16a61a6ffadf6dca4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/543M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"768f249be3cc4a48bfa0004f7129764b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/895k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b28f96014ef14719a75a363825e2a563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc0f939e158e4526a4d7b28f6e3e4e16"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["#roberta_base = RobertaForMaskedLM.from_pretrained('roberta-base')\n","roberta_base = RobertaForMaskedLM.from_pretrained(\"vinai/phobert-base\")\n","# roberta_base_tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n","roberta_base_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"SrkrnobWvy2f","executionInfo":{"status":"ok","timestamp":1666183813526,"user_tz":-420,"elapsed":19,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"}}},"outputs":[],"source":["class RobertaLongSelfAttention(LongformerSelfAttention):\n","    def forward(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        head_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        past_key_value = None,\n","        output_attentions=False,\n","    ):\n","        attention_mask = attention_mask.squeeze(dim=2).squeeze(dim=1)\n","        is_index_masked = attention_mask < 0\n","        is_index_global_attn = attention_mask > 0\n","        is_global_attn = any(is_index_global_attn.flatten())\n","        return super().forward(hidden_states, \n","                               is_index_masked=is_index_masked, \n","                               is_index_global_attn=is_index_global_attn, \n","                               is_global_attn=is_global_attn,\n","                               attention_mask=attention_mask, \n","                               output_attentions=output_attentions)\n","\n","class RobertaLongForMaskedLM(RobertaForMaskedLM):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        for i, layer in enumerate(self.roberta.encoder.layer):\n","            layer.attention.self = RobertaLongSelfAttention(config, layer_id=i)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"dD5fXKQQwJrj","executionInfo":{"status":"ok","timestamp":1666183813527,"user_tz":-420,"elapsed":17,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"}}},"outputs":[],"source":["def create_long_model(save_model_to, attention_window, max_pos):\n","    model = roberta_base\n","    tokenizer = roberta_base_tokenizer\n","    config = model.config\n","\n","    # extend position embeddings\n","    tokenizer.model_max_length = max_pos\n","    tokenizer.init_kwargs['model_max_length'] = max_pos\n","    current_max_pos, embed_size = model.roberta.embeddings.position_embeddings.weight.shape\n","    max_pos += 2  # NOTE: RoBERTa has positions 0,1 reserved, so embedding size is max position + 2\n","    config.max_position_embeddings = max_pos\n","    assert max_pos > current_max_pos\n","    # allocate a larger position embedding matrix\n","    new_pos_embed = model.roberta.embeddings.position_embeddings.weight.new_empty(max_pos, embed_size)\n","    # copy position embeddings over and over to initialize the new position embeddings\n","    k = 2\n","    step = current_max_pos - 2\n","    while k < max_pos - 1:\n","        new_pos_embed[k:(k + step)] = model.roberta.embeddings.position_embeddings.weight[2:]\n","        k += step\n","    model.roberta.embeddings.position_embeddings.weight.data = new_pos_embed\n","    model.roberta.embeddings.position_ids.data = torch.tensor([i for i in range(max_pos)]).reshape(1, max_pos)\n","\n","    # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n","    config.attention_window = [attention_window] * config.num_hidden_layers\n","    for i, layer in enumerate(model.roberta.encoder.layer):\n","        longformer_self_attn = LongformerSelfAttention(config, layer_id=i)\n","        longformer_self_attn.query = layer.attention.self.query\n","        longformer_self_attn.key = layer.attention.self.key\n","        longformer_self_attn.value = layer.attention.self.value\n","\n","        longformer_self_attn.query_global = copy.deepcopy(layer.attention.self.query)\n","        longformer_self_attn.key_global = copy.deepcopy(layer.attention.self.key)\n","        longformer_self_attn.value_global = copy.deepcopy(layer.attention.self.value)\n","\n","        layer.attention.self = longformer_self_attn\n","\n","    logger.info(f'saving model to {save_model_to}')\n","    model.save_pretrained(save_model_to)\n","    tokenizer.save_pretrained(save_model_to)\n","    return model, tokenizer"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"dxLmgfqywLGo","executionInfo":{"status":"ok","timestamp":1666183813528,"user_tz":-420,"elapsed":17,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"}}},"outputs":[],"source":["def copy_proj_layers(model):\n","    for i, layer in enumerate(model.roberta.encoder.layer):\n","        layer.attention.self.query_global = copy.deepcopy(layer.attention.self.query)\n","        layer.attention.self.key_global = copy.deepcopy(layer.attention.self.key)\n","        layer.attention.self.value_global = copy.deepcopy(layer.attention.self.value)\n","    return model"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"KWciTb7OwNU5","executionInfo":{"status":"ok","timestamp":1666183819606,"user_tz":-420,"elapsed":568,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"}}},"outputs":[],"source":["def pretrain_and_evaluate(args, model, tokenizer, eval_only, model_path):\n","    val_dataset = TextDataset(tokenizer=tokenizer,\n","                              file_path=args.val_datapath,\n","                              block_size=tokenizer.model_max_length)\n","    if eval_only:\n","        train_dataset = val_dataset\n","    else:\n","        logger.info(f'Loading and tokenizing training data is usually slow: {args.train_datapath}')\n","        train_dataset = TextDataset(tokenizer=tokenizer,\n","                                    file_path=args.train_datapath,\n","                                    block_size=tokenizer.model_max_length)\n","\n","    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n","    trainer = Trainer(model=model, args=args, data_collator=data_collator,\n","                      train_dataset=train_dataset, eval_dataset=val_dataset)\n","\n","    eval_loss = trainer.evaluate()\n","    eval_loss = eval_loss['eval_loss']\n","    logger.info(f'Initial eval bpc: {eval_loss/math.log(2)}')\n","    #print(eval_loss/math.log(2))\n","    \n","    if not eval_only:\n","        trainer.train(model_path=model_path)\n","        trainer.save_model()\n","\n","        eval_loss = trainer.evaluate()\n","        eval_loss = eval_loss['eval_loss']\n","        #print(eval_loss/math.log(2))\n","        logger.info(f'Eval bpc after pretraining: {eval_loss/math.log(2)}')\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"_ZyNwMGhwS9g","executionInfo":{"status":"ok","timestamp":1666183813530,"user_tz":-420,"elapsed":17,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"}}},"outputs":[],"source":["@dataclass\n","class ModelArgs:\n","    attention_window: int = field(default=256, metadata={\"help\": \"Size of attention window\"})\n","    max_pos: int = field(default=1024, metadata={\"help\": \"Maximum position\"})\n","\n","parser = HfArgumentParser((TrainingArguments, ModelArgs,))\n","\n","\n","training_args, model_args = parser.parse_args_into_dataclasses(look_for_args_file=False, args=[\n","    '--output_dir', 'tmp',\n","    '--warmup_steps', '500',\n","    '--learning_rate', '0.00003',\n","    '--weight_decay', '0.01',\n","    '--adam_epsilon', '1e-6',\n","    '--max_steps', '3000',\n","    '--logging_steps', '500',\n","    '--save_steps', '500',\n","    '--max_grad_norm', '5.0',\n","    '--per_device_eval_batch_size', '8',\n","    '--per_device_train_batch_size', '2',  # 32GB gpu with fp32\n","    '--gradient_accumulation_steps', '32',\n","#    '--evaluate_during_training',\n","    '--do_train',\n","    '--do_eval',\n","    '--prediction_loss_only', 'True'\n","])\n","training_args.val_datapath = '/content/drive/MyDrive/z_inf_data/word_seg_corpus_valid.txt'\n","training_args.train_datapath = '/content/drive/MyDrive/z_inf_data/word_seg_corpus_test.txt'\n","\n","# Choose GPU\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36049,"status":"ok","timestamp":1666108454736,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"},"user_tz":-420},"id":"AM89KOF3wWXo","outputId":"0a522593-de6c-4ce0-c7d9-c44ea0898dc8"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:__main__:Evaluating Phobert (seqlen: 256) for refernece ...\n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","max_steps is given, it will override any value given in num_train_epochs\n"]}],"source":["logger.info('Evaluating Phobert (seqlen: 256) for refernece ...')\n","pretrain_and_evaluate(training_args, roberta_base, roberta_base_tokenizer, eval_only=True, model_path=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1343,"status":"ok","timestamp":1666115129888,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"},"user_tz":-420},"id":"Zht4Zz1T8YqG","outputId":"47055d54-1e54-4cb9-8858-d828b4710d69"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:__main__:Converting phobert into phobert-1024\n","INFO:__main__:saving model to tmp/phobert-1024\n"]}],"source":["model_path = f'{training_args.output_dir}/phobert-{model_args.max_pos}'\n","if not os.path.exists(model_path):\n","    os.makedirs(model_path)\n","\n","logger.info(f'Converting phobert into phobert-{model_args.max_pos}')\n","model, tokenizer = create_long_model(\n","    save_model_to=model_path, attention_window=model_args.attention_window, max_pos=model_args.max_pos)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2167,"status":"ok","timestamp":1666115132013,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"},"user_tz":-420},"id":"EyYzaVhMc82e","outputId":"949c9840-0cd0-4e8e-b269-db2fe83714af"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:__main__:Loading the model from tmp/phobert-1024\n"]}],"source":["logger.info(f'Loading the model from {model_path}')\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = RobertaLongForMaskedLM.from_pretrained(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"elapsed":1296898,"status":"ok","timestamp":1666087951393,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"},"user_tz":-420},"id":"zulSCpP7dNrv","outputId":"69215e4b-6cf2-4fa2-d2ab-1875fcf079bc"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","Creating features from dataset file at /content/drive/MyDrive/z_inf_data\n","Saving features into cached file /content/drive/MyDrive/z_inf_data/cached_lm_PhobertTokenizer_1022_word_seg_corpus_valid.txt [took 0.176 s]\n","max_steps is given, it will override any value given in num_train_epochs\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n","Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n","***** Running Evaluation *****\n","  Num examples = 5572\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='697' max='697' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [697/697 21:06]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"]},{"name":"stdout","output_type":"stream","text":["2.2595022196254506\n"]}],"source":["pretrain_and_evaluate(training_args, model, tokenizer, eval_only=True, model_path=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"fL_As8RZmiDb","outputId":"d4896332-c9f8-491d-f186-a75a2e63d579"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:__main__:Pretraining phobert-1024 ... \n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","INFO:__main__:Loading and tokenizing training data is usually slow: /content/drive/MyDrive/z_inf_data/word_seg_corpus_test.txt\n","max_steps is given, it will override any value given in num_train_epochs\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1464: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  FutureWarning,\n","Loading model from /content/tmp/phobert-1024.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 263268\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 32\n","  Total optimization steps = 2000\n","Didn't find an RNG file, if you are resuming a training that was launched in a distributed fashion, reproducibility is not guaranteed.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2000/2000 17:26:55, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.448900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.366300</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.352700</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.344200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to tmp/checkpoint-500\n","Configuration saved in tmp/checkpoint-500/config.json\n","Model weights saved in tmp/checkpoint-500/pytorch_model.bin\n","Saving model checkpoint to tmp/checkpoint-1000\n","Configuration saved in tmp/checkpoint-1000/config.json\n","Model weights saved in tmp/checkpoint-1000/pytorch_model.bin\n","Saving model checkpoint to tmp/checkpoint-1500\n","Configuration saved in tmp/checkpoint-1500/config.json\n","Model weights saved in tmp/checkpoint-1500/pytorch_model.bin\n","Saving model checkpoint to tmp/checkpoint-2000\n","Configuration saved in tmp/checkpoint-2000/config.json\n","Model weights saved in tmp/checkpoint-2000/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to tmp\n","Configuration saved in tmp/config.json\n","Model weights saved in tmp/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 5572\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='697' max='697' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [697/697 26:06]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:__main__:Eval bpc after pretraining: 1.8116716075469081\n"]}],"source":["logger.info(f'Pretraining phobert-{model_args.max_pos} ... ')\n","\n","training_args.max_steps = 2000   ## <<<<<<<<<<<<<<<<<<<<<<<< REMOVE THIS <<<<<<<<<<<<<<<<<<<<<<<<\n","\n","pretrain_and_evaluate(training_args, model, tokenizer, eval_only=False, model_path='/content/tmp/phobert-1024')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"G-h-zY23n3BM","outputId":"f7c8442a-dc4c-45da-b817-fc5186b7aa30"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:__main__:Copying local projection layers into global projection layers ... \n","INFO:__main__:Saving model to tmp/phobert-1024\n","Configuration saved in tmp/phobert-1024/config.json\n","Model weights saved in tmp/phobert-1024/pytorch_model.bin\n"]}],"source":["logger.info(f'Copying local projection layers into global projection layers ... ')\n","model = copy_proj_layers(model)\n","logger.info(f'Saving model to {model_path}')\n","model.save_pretrained(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":405,"status":"ok","timestamp":1666090061731,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"},"user_tz":-420},"id":"WCC0y4GQsRkv","outputId":"fdb39658-c239-4a7c-ee39-9ca49b1cb8e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU 0: Tesla T4 (UUID: GPU-20d1c5f5-9552-c9a7-b438-dddf73519b13)\n"]}],"source":["!nvidia-smi -L"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"k72LJL0iK6C-","outputId":"df5f7c8c-1360-4ebb-86a2-901bf96fe905"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/phobert-1024-2000steps.zip'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["import shutil\n","shutil.make_archive('phobert-1024-2000steps', 'zip', '/content/tmp/phobert-1024')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CtS3jx7TLGLx"},"outputs":[],"source":["!cp /content/phobert-1024-2000steps.zip /content/drive/MyDrive/z_inf_data"]},{"cell_type":"code","source":["import zipfile\n","with zipfile.ZipFile('/content/drive/MyDrive/z_inf_data/phobert-1024-2000steps.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/content/phobert')"],"metadata":{"id":"P5RR-X0dS-y0","executionInfo":{"status":"ok","timestamp":1666183901192,"user_tz":-420,"elapsed":7334,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained('/content/phobert')\n","model = RobertaLongForMaskedLM.from_pretrained('/content/phobert')"],"metadata":{"id":"SDqBh0YqS61j","executionInfo":{"status":"ok","timestamp":1666183915036,"user_tz":-420,"elapsed":2985,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["pretrain_and_evaluate(training_args, model, tokenizer, eval_only=True, model_path=None)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179},"id":"_CQ_qG10TN3K","executionInfo":{"status":"ok","timestamp":1666185211970,"user_tz":-420,"elapsed":1263709,"user":{"displayName":"S∆°n Lam Nguy·ªÖn ƒê·∫∑ng","userId":"11953831021002530128"}},"outputId":"b0c3333f-baf6-4e72-f24b-0dd347fc58ea"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","max_steps is given, it will override any value given in num_train_epochs\n","***** Running Evaluation *****\n","  Num examples = 5572\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='697' max='697' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [697/697 20:51]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:Initial eval bpc: 1.8153857448749413\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMCzaxypcTpRJxhMJi8AQBj"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a303469db2d64d16a61a6ffadf6dca4f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ffc9e3081f8e465d9593d5bc4155f9b9","IPY_MODEL_e0d10dcfb5ba4415badf770df6a2a83d","IPY_MODEL_296cf74359964a5da8539222b555f2fa"],"layout":"IPY_MODEL_6fb3b281bb3f4245af22d4ff9444db91"}},"ffc9e3081f8e465d9593d5bc4155f9b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c69c36cc1e4d42ee9b969477a351394e","placeholder":"‚Äã","style":"IPY_MODEL_1efea6b592bc4146aab2251d36af5f7f","value":"Downloading: 100%"}},"e0d10dcfb5ba4415badf770df6a2a83d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f196c9c905394386a4541ac5fd826c8f","max":557,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e06499e927a4cdaba6ed7957ff9e402","value":557}},"296cf74359964a5da8539222b555f2fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85f6601a3772461895722c0d58a8bbe9","placeholder":"‚Äã","style":"IPY_MODEL_f06c1f6b5f5d49b5832e950deaa23203","value":" 557/557 [00:00&lt;00:00, 19.7kB/s]"}},"6fb3b281bb3f4245af22d4ff9444db91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c69c36cc1e4d42ee9b969477a351394e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1efea6b592bc4146aab2251d36af5f7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f196c9c905394386a4541ac5fd826c8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e06499e927a4cdaba6ed7957ff9e402":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85f6601a3772461895722c0d58a8bbe9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f06c1f6b5f5d49b5832e950deaa23203":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"768f249be3cc4a48bfa0004f7129764b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_55637dcaa88f439788d3b69919d4a2e9","IPY_MODEL_fbcefcf8d7dc4aa88e0129b14dfad2bd","IPY_MODEL_1e0c6d23539847a1977f222ca785144e"],"layout":"IPY_MODEL_081e8cc2d0d3407587e24d29b6b31fd4"}},"55637dcaa88f439788d3b69919d4a2e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d198507b83c24dbe874f3c1b5898d314","placeholder":"‚Äã","style":"IPY_MODEL_3ddf5f077895482a8b0f613e159c8b7d","value":"Downloading: 100%"}},"fbcefcf8d7dc4aa88e0129b14dfad2bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_457a4618851546c4a0a015284e08e880","max":542923308,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4bf25c6e3fd34c0d909ca65e7d79eb90","value":542923308}},"1e0c6d23539847a1977f222ca785144e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a66640340c20455aad7f2122a75d0e67","placeholder":"‚Äã","style":"IPY_MODEL_f4da6207d39a4f5ba8534b0f1c7d04c3","value":" 543M/543M [00:08&lt;00:00, 70.6MB/s]"}},"081e8cc2d0d3407587e24d29b6b31fd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d198507b83c24dbe874f3c1b5898d314":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ddf5f077895482a8b0f613e159c8b7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"457a4618851546c4a0a015284e08e880":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bf25c6e3fd34c0d909ca65e7d79eb90":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a66640340c20455aad7f2122a75d0e67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4da6207d39a4f5ba8534b0f1c7d04c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b28f96014ef14719a75a363825e2a563":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75096a73ba4c48e49c5cfb73a57ff613","IPY_MODEL_491f722ec598490bbe5af9f0d45699d4","IPY_MODEL_73e11c0b37f04caf80007c8f4d50cdde"],"layout":"IPY_MODEL_6b172bf44f7e422a9fc634aa5144db58"}},"75096a73ba4c48e49c5cfb73a57ff613":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55327c2fe9bb4b5c88a394ec317ec9a4","placeholder":"‚Äã","style":"IPY_MODEL_39719bd81ec74d8088343df0f9f47023","value":"Downloading: 100%"}},"491f722ec598490bbe5af9f0d45699d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_916b3ba5b3564025b8700de93d7afc66","max":895321,"min":0,"orientation":"horizontal","style":"IPY_MODEL_01c62d0f559f4b3b860a5754027513a6","value":895321}},"73e11c0b37f04caf80007c8f4d50cdde":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ffbc99d12ef4985b31fafb25154b28f","placeholder":"‚Äã","style":"IPY_MODEL_9208c5eb32bb45d488d3661a70ee0b95","value":" 895k/895k [00:01&lt;00:00, 1.02MB/s]"}},"6b172bf44f7e422a9fc634aa5144db58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55327c2fe9bb4b5c88a394ec317ec9a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39719bd81ec74d8088343df0f9f47023":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"916b3ba5b3564025b8700de93d7afc66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01c62d0f559f4b3b860a5754027513a6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ffbc99d12ef4985b31fafb25154b28f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9208c5eb32bb45d488d3661a70ee0b95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc0f939e158e4526a4d7b28f6e3e4e16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd016e04ee18492fbb303fb4d71bc8dd","IPY_MODEL_1bb365e6840c4dc88bca7de3bc8fe93b","IPY_MODEL_3d98f4e3067c4ab6b5fb647b508a107b"],"layout":"IPY_MODEL_edee4039146e44e881ea36c8d0938c5a"}},"fd016e04ee18492fbb303fb4d71bc8dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_faeeb42120314eb99a1dea8daf338257","placeholder":"‚Äã","style":"IPY_MODEL_c34f79e2617045e6bf94b47df031f3b1","value":"Downloading: 100%"}},"1bb365e6840c4dc88bca7de3bc8fe93b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2416d1a36eb54b74836a69a15ccf220d","max":1135173,"min":0,"orientation":"horizontal","style":"IPY_MODEL_18ba6524105f43f9955563e2fd04afd8","value":1135173}},"3d98f4e3067c4ab6b5fb647b508a107b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_599b7897741c4791a7316d83d81b0fba","placeholder":"‚Äã","style":"IPY_MODEL_7e9f6134a1bd468d8477194d32074906","value":" 1.14M/1.14M [00:01&lt;00:00, 975kB/s]"}},"edee4039146e44e881ea36c8d0938c5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"faeeb42120314eb99a1dea8daf338257":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c34f79e2617045e6bf94b47df031f3b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2416d1a36eb54b74836a69a15ccf220d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18ba6524105f43f9955563e2fd04afd8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"599b7897741c4791a7316d83d81b0fba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e9f6134a1bd468d8477194d32074906":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}