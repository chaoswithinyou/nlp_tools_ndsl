{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOGE5tC0kzOMzh4Dltmx9//"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from IPython.display import clear_output "],"metadata":{"id":"A0k6CaS9oj7S","executionInfo":{"status":"ok","timestamp":1666022119522,"user_tz":-420,"elapsed":15,"user":{"displayName":"Sơn Lam Nguyễn Đặng","userId":"11953831021002530128"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TPkFOBZJmiBE","executionInfo":{"status":"ok","timestamp":1666022120944,"user_tz":-420,"elapsed":1434,"user":{"displayName":"Sơn Lam Nguyễn Đặng","userId":"11953831021002530128"}},"outputId":"75737ff5-5fb9-4597-f5c4-191b90cbbb59"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'bert-extractive-summarization'...\n","remote: Enumerating objects: 323, done.\u001b[K\n","remote: Counting objects: 100% (120/120), done.\u001b[K\n","remote: Compressing objects: 100% (52/52), done.\u001b[K\n","remote: Total 323 (delta 90), reused 89 (delta 67), pack-reused 203\u001b[K\n","Receiving objects: 100% (323/323), 341.72 KiB | 2.39 MiB/s, done.\n","Resolving deltas: 100% (168/168), done.\n"]}],"source":["!git clone --branch fourbert --single-branch https://github.com/chaoswithinyou/bert-extractive-summarization"]},{"cell_type":"code","source":["%cd bert-extractive-summarization"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5s36auCGoeFQ","executionInfo":{"status":"ok","timestamp":1666022120945,"user_tz":-420,"elapsed":15,"user":{"displayName":"Sơn Lam Nguyễn Đặng","userId":"11953831021002530128"}},"outputId":"3fa69cb4-a451-4f07-96a0-cd1e9d3ecabd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/bert-extractive-summarization\n"]}]},{"cell_type":"code","source":["!python /content/bert-extractive-summarization/prepare.py\n","clear_output()"],"metadata":{"id":"G9nzueLtoieH","executionInfo":{"status":"ok","timestamp":1666022253493,"user_tz":-420,"elapsed":132555,"user":{"displayName":"Sơn Lam Nguyễn Đặng","userId":"11953831021002530128"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!python /content/bert-extractive-summarization/infer_from_url.py -url https://vietnamnet.vn/tong-bi-thu-thu-tuong-va-chu-tich-quoc-hoi-tiep-tong-thong-singapore-2071012.html"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w7J8Z5bQomgi","executionInfo":{"status":"ok","timestamp":1666022562648,"user_tz":-420,"elapsed":21246,"user":{"displayName":"Sơn Lam Nguyễn Đặng","userId":"11953831021002530128"}},"outputId":"e74ecdfd-967b-4fea-fa70-71f88fde367e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-10-17 16:02:23 INFO  WordSegmenter:24 - Loading Word Segmentation model\n","Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/content/bert-extractive-summarization/models/neural.py:168: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1581.)\n","  scores = scores.masked_fill(mask.byte(), -1e18)\n","[[1.3124595 1.1752285 1.1268789 1.1042867 1.0900847 1.0705212 1.0693607\n","  1.0696055 1.0692159 1.0684123 1.0699621 1.0675198]]\n","[[ 0  1  2  3  4  5 10  7  6  8  9 11]]\n","Description:\n","\n","Tổng Bí thư Nguyễn Phú Trọng, Thủ tướng Phạm Minh Chính và Chủ tịch Quốc hội Vương Đình Huệ đã tiếp Tổng thống Singapore Halimah Yacob thăm cấp Nhà nước Việt Nam.\n","\n","Important sentences:\n","\n","Các nhà lãnh đạo Việt Nam hoan nghênh Tổng thống Halimah Yacob và đoàn đại biểu cấp cao Singapore thăm chính thức cấp Nhà nước đến Việt Nam , cho rằng chuyến thăm là biểu hiện sinh động của mối quan hệ hữu nghị và hợp tác tốt đẹp giữa Việt Nam và Singapore , đặt dấu mốc mới đưa quan hệ giữa hai nước ngày càng đi vào chiều sâu trong bối cảnh tiến tới kỷ niệm 50 năm thiết lập quan hệ ngoại giao và 10 năm thiết lập quan hệ Đối tác chiến lược vào năm 2023 .\n","\n","Tổng Bí thư Nguyễn Phú Trọng tiếp Tổng thống Singapore .\n","\n","Singapore luôn coi trọng phát triển quan hệ với Việt Nam , phát huy những thành tựu đã đạt được cũng như mở rộng hợp tác trong giai đoạn mới trên các lĩnh vực như năng lượng tái tạo , an ninh mạng , thương mại và đầu tư , hợp tác đào tạo nguồn nhân lực , trong đó có có chương trình đào tạo dành cho cán bộ cấp cao của Đảng Cộng sản Việt Nam tại Singapore .\n","\n","Tổng thống Halimah Yacob mong muốn hai bên phối hợp chặt chẽ để triển khai có hiệu quả các thoả thuận hợp tác , đưa quan hệ hai nước phát triển thực chất và hiệu quả , đem lại lợi ích cho nhân dân hai nước , trở thành hình mẫu hợp tác trong các nước ASEAN .\n","\n","Tiến tới xây dựng Quan hệ Đối tác kinh tế số - kinh tế xanh Tại cuộc gặp với Thủ tướng Phạm Minh Chính , Tổng thống Halimah Yacob chuyển lời thăm hỏi và lời mời thăm Singapore của Thủ tướng Lý Hiển Long tới Thủ tướng Phạm Minh Chính .\n"]}]}]}